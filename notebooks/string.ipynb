{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import vmap\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from math import pi\n",
    "import time\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "import interflow as itf\n",
    "import interflow.fabrics as fabrics\n",
    "import interflow.gmm as gmm\n",
    "import interflow.stochastic_interpolant as stochastic_interpolant\n",
    "import torch.distributions as D\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print('Using GPU.')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using the cpu. No GPU!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab(tensor):\n",
    "    return tensor.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(5.)\n",
    "mu0 = torch.tensor([0.,0.]).to(device)\n",
    "mu1 = torch.tensor([a - (2),a]).to(device)\n",
    "C0 = torch.tensor([[1.,0.],[0.,a ]]).to(device)#.unsqueeze(0)\n",
    "C1 = torch.tensor([[a, 0.],[0 ,1.]]).to(device)#.unsqueeze(0)\n",
    "\n",
    "\n",
    "rho0 = D.MultivariateNormal(mu0,C0)\n",
    "rho1 = D.MultivariateNormal(mu1, C1)       \n",
    "rho  = D.MixtureSameFamily(\n",
    "    D.Categorical(torch.tensor([0.5, 0.5]).to(device)),\n",
    "    D.MultivariateNormal(torch.stack([mu0, mu1]), torch.stack([C0, C1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = grab(rho.sample((10000,)))\n",
    "plt.scatter(test_samples[:, 0], test_samples[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho0_samples = grab(rho0.sample((10000,)))\n",
    "rho1_samples = grab(rho1.sample((10000,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(rho0_samples[:,0], rho0_samples[:,1], alpha = 0.5)\n",
    "plt.scatter(rho1_samples[:,0], rho1_samples[:,1], alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define score for mxiture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_score(mu0, mu1, C0, C1):\n",
    "    \n",
    "    def s(x):\n",
    "        C0_inv = torch.linalg.inv(C0)\n",
    "        C1_inv = torch.linalg.inv(C1)\n",
    "        N0     = (2*pi)**(-1)* torch.linalg.det(C0)**(-1/2)*torch.exp(-(1/2)*(x-mu0) @ C0_inv @ (x-mu0))\n",
    "        N1     = (2*pi)**(-1)* torch.linalg.det(C1)**(-1/2)*torch.exp(-(1/2)*(x-mu1) @ C1_inv @ (x-mu1))\n",
    "        \n",
    "        return (-C0_inv@(x-mu0)*N0 - C1_inv @(x-mu1)*N1)/(N0 + N1)\n",
    "\n",
    "    return s\n",
    "\n",
    "def make_log_prob(mu0, mu1, C0, C1):\n",
    "    C0_inv = torch.linalg.inv(C0)\n",
    "    C1_inv = torch.linalg.inv(C1)\n",
    "    \n",
    "    def log_prob(x):\n",
    "        N0     = (2*pi)**(-1)* torch.linalg.det(C0)**(-1/2)*torch.exp(-(1/2)*(x-mu0) @ C0_inv @ (x-mu0))\n",
    "        N1     = (2*pi)**(-1)* torch.linalg.det(C1)**(-1/2)*torch.exp(-(1/2)*(x-mu1) @ C1_inv @ (x-mu1))\n",
    "        return torch.log( (1/2) * (N0 + N1))\n",
    "    return log_prob\n",
    "\n",
    "s = make_score(mu0, mu1, C0, C1)\n",
    "s_batch = vmap(s)\n",
    "xs = torch.randn(size=(10,2)).to(device)\n",
    "s_batch(xs).shape\n",
    "\n",
    "log_p = make_log_prob(mu0, mu1, C0, C1)\n",
    "log_p_batch = vmap(log_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a meshgrid\n",
    "x = torch.linspace(-10, 10, 25)\n",
    "y = torch.linspace(-10, 10, 25)\n",
    "xx, yy = torch.meshgrid(x, y)\n",
    "grid = torch.stack([xx.flatten(), yy.flatten()], dim=1).to(device)\n",
    "print(grid.shape)\n",
    "\n",
    "# Evaluate the function on the grid\n",
    "results = s_batch(grid)\n",
    "\n",
    "# Reshape for plotting\n",
    "u = results[:, 0].reshape(xx.shape)\n",
    "v = results[:, 1].reshape(yy.shape)\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(rho0_samples[:,0], rho0_samples[:,1], s = 2, alpha = 0.2)\n",
    "plt.scatter(rho1_samples[:,0], rho1_samples[:,1], s = 2, alpha = 0.2)\n",
    "plt.quiver(grab(xx), grab(yy), grab(u), grab(v), zorder=1)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Vector Field of s(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### unbatched\n",
    "\n",
    "# import torch\n",
    "\n",
    "# def interp_at_t(strings, ts, tprime):\n",
    "#     # Ensure ts is sorted\n",
    "#     # ts, sorted_indices = torch.sort(ts)\n",
    "#     # strings = strings[sorted_indices]\n",
    "\n",
    "#     # Searchsorted finds the indices where elements should be inserted to maintain order.\n",
    "#     index = torch.searchsorted(ts, tprime, right=True)\n",
    "\n",
    "#     # Handle edge cases\n",
    "#     index = index.clamp(1, len(ts) - 1)\n",
    "\n",
    "#     string1 = strings[index]\n",
    "#     string0 = strings[index - 1]\n",
    "#     ts1 = ts[index]\n",
    "#     ts0 = ts[index - 1]\n",
    "\n",
    "#     # Linear interpolation\n",
    "#     return string0 + (tprime - ts0) / (ts1 - ts0) * (string1 - string0)\n",
    "\n",
    "# # Example usage\n",
    "# ts = torch.tensor([0, 0.5, 1.0])\n",
    "# strings = torch.tensor([[0, 0], [0.5, torch.sqrt(torch.tensor(3/4))], [1, 0]])\n",
    "# tprimes = torch.tensor([0.25, 0.75])\n",
    "\n",
    "# # Assuming you have some vmap equivalent in PyTorch or using another framework\n",
    "# # interpolated_values = [interp_at_t(strings, ts, tprime) for tprime in tprimes]\n",
    "\n",
    "# # If you don't have vmap, you can use a simple list comprehension\n",
    "# interpolated_values = torch.stack([interp_at_t(strings, ts, tprime) for tprime in tprimes])\n",
    "\n",
    "# print(interpolated_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_interp_at_t(strings, ts, tprimes):\n",
    "    # Ensure ts is sorted\n",
    "    ts, sorted_indices = torch.sort(ts)\n",
    "    strings = strings[sorted_indices]\n",
    "\n",
    "    # Searchsorted finds the indices where elements should be inserted to maintain order.\n",
    "    indices = torch.searchsorted(ts, tprimes, right=True)\n",
    "\n",
    "    # Handle edge cases\n",
    "    indices = indices.clamp(1, len(ts) - 1)\n",
    "\n",
    "    string1 = strings[indices]\n",
    "    string0 = strings[indices - 1]\n",
    "    ts1 = ts[indices]\n",
    "    ts0 = ts[indices - 1]\n",
    "\n",
    "    # Calculate the interpolation\n",
    "    interp = string0 + (tprimes.unsqueeze(-1) - ts0.unsqueeze(-1)) / (ts1 - ts0).unsqueeze(-1) * (string1 - string0)\n",
    "\n",
    "    return interp\n",
    "\n",
    "# Testing the batched method\n",
    "ts = torch.tensor([0, 0.5, 1.0])\n",
    "strings = torch.tensor([[0, 0], [0.5, torch.sqrt(torch.tensor(3/4))], [1, 0]])\n",
    "tprimes = torch.tensor([0.25, 0.75])\n",
    "\n",
    "batched_results = batched_interp_at_t(strings, ts, tprimes)\n",
    "print(\"Batched Results:\\n\", batched_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_at_t(strings, ts, tprime):\n",
    "    \n",
    "    print((ts > tprime).shape)\n",
    "    index = torch.argmax((ts > tprime).type(torch.float32))\n",
    "    string1 = strings[index]\n",
    "    string0 = strings[index - 1]\n",
    "    ts1     = ts[index]\n",
    "    ts0     = ts[index - 1]\n",
    "    \n",
    "    return string0 + (tprime - ts0)/(ts1 - ts0) * (string1 - string0)\n",
    "\n",
    "def batched_interp_at_t(strings, ts, tprimes):\n",
    "    # Ensure ts is sorted\n",
    "    ts, sorted_indices = torch.sort(ts)\n",
    "    strings = strings[sorted_indices]\n",
    "\n",
    "    # Searchsorted finds the indices where elements should be inserted to maintain order.\n",
    "    indices = torch.searchsorted(ts, tprimes, right=True)\n",
    "\n",
    "    # Handle edge cases\n",
    "    indices = indices.clamp(1, len(ts) - 1)\n",
    "\n",
    "    string1 = strings[indices]\n",
    "    string0 = strings[indices - 1]\n",
    "    ts1 = ts[indices]\n",
    "    ts0 = ts[indices - 1]\n",
    "\n",
    "    # Calculate the interpolation\n",
    "    interp = string0 + (tprimes.unsqueeze(-1) - ts0.unsqueeze(-1)) / (ts1 - ts0).unsqueeze(-1) * (string1 - string0)\n",
    "\n",
    "    return interp\n",
    "\n",
    "def step_string(\n",
    "    string: torch.tensor,\n",
    "    score: Callable,\n",
    "    dt: float,\n",
    "):\n",
    "    n_disc = string.shape[0]\n",
    "    \n",
    "    # string: [n_disc, d] (per string)\n",
    "    string[1:-1] = string[1:-1] + dt*score(string[1:-1])\n",
    "    \n",
    "    # compute_lengths\n",
    "    dists = torch.sum((string[1:].reshape(n_disc-1, -1) - string[:-1].reshape(n_disc-1, -1))**2, axis=1)\n",
    "    # print(\"DISTS:\", dists.shape)\n",
    "    sum_dists = torch.cumsum(dists, dim=0)\n",
    "    sum_dists = sum_dists/ sum_dists[-1]\n",
    "    \n",
    "    sum_dists = torch.concatenate((torch.tensor((0.0,)).to(device), sum_dists), dim=0)\n",
    "    # print(sum_dists)\n",
    "    \n",
    "    uniform_grid = torch.linspace(0.0, 1.0, n_disc).to(device)\n",
    "    \n",
    "    # print(len(sum_dists))\n",
    "    return batched_interp_at_t(string, sum_dists, uniform_grid) # uniform_grid is new tprimes\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.tensor([0, -4.0]).to(device)\n",
    "x1 = torch.tensor([5., 5.0]).to(device)\n",
    "print(x0, x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(grab(x0[0]), grab(x0[1]))\n",
    "plt.scatter(grab(x1[0]), grab(x1[1]))\n",
    "plt.xlim(-5,8)\n",
    "plt.ylim(-7,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string = torch.stack([x0*(1-t) + t*(x1) for t in ts]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.linspace(0,1,100).to(device)\n",
    "string = torch.stack([x0*(1-t) + t*(x1) for t in ts]).to(device)\n",
    "n_steps = 10\n",
    "dt = .5\n",
    "\n",
    "strings_over_time = []\n",
    "new_string = string\n",
    "strings_over_time.append(new_string)\n",
    "for step in range(n_steps):\n",
    "    new_string = step_string(new_string, s_batch, dt)\n",
    "    strings_over_time.append(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_string = grab(strings_over_time[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_color_sequence(start_color, end_color, n_steps):\n",
    "    \"\"\"Generate a sequence of colors from start_color to end_color.\"\"\"\n",
    "    return [start_color + (end_color - start_color) * i / (n_steps - 1) for i in range(n_steps)]\n",
    "\n",
    "# Define the start (light gray) and end (green) colors in RGB\n",
    "start_color = np.array([200, 200, 200]) / 255  # Light gray\n",
    "end_color = np.array([0, 100, 20]) / 255       # Green\n",
    "\n",
    "# Number of steps/colors you want in your sequence\n",
    "steps = 1000\n",
    "\n",
    "# Generate the color sequence\n",
    "color_sequence = create_color_sequence(start_color, end_color, steps)\n",
    "\n",
    "\n",
    "for i,string in enumerate(strings_over_time[:steps]):\n",
    "    string = grab(string)\n",
    "    # print(string)\n",
    "    plt.scatter(string[:,0], string[:,1], color=color_sequence[i], alpha=0.5, s = 1)\n",
    "    if i > steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_string = grab(strings_over_time[80])\n",
    "# # final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-5, 10, 100)\n",
    "y = torch.linspace(-8, 10, 100)\n",
    "X, Y = torch.meshgrid(x, y)\n",
    "\n",
    "# Step 3: Transform the grid points into vectors and evaluate the function\n",
    "Z = torch.empty(X.shape)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = log_p(torch.tensor([X[i, j], Y[i, j]]).to(device) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Plot the contour\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.contourf(X, Y, Z, levels=50, cmap='viridis')\n",
    "plt.colorbar(label='Log probability')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Contour Plot of Log Probability Function')\n",
    "\n",
    "for i,string in enumerate(strings_over_time[::1][:steps]):\n",
    "    string = grab(string)\n",
    "    # print(string)\n",
    "    plt.scatter(string[:,0], string[:,1], color=color_sequence[::1][i], alpha=0.5, s = 1)\n",
    "    if i > steps:\n",
    "        break\n",
    "        \n",
    "        \n",
    "plt.scatter(grab(x0[0]), grab(x0[1]), label = 'x0', s = 10, color = 'blue')\n",
    "plt.scatter(grab(x1[0]), grab(x1[1]), label = 'x1', s = 10, color = 'blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fc_net(\n",
    "    hidden_sizes: int, \n",
    "    in_size: int, \n",
    "    out_size: int, \n",
    "    inner_act: str, \n",
    "    final_act: str, \n",
    "    **config\n",
    "):\n",
    "    \"\"\"Construct a fully-connected network.\"\"\"\n",
    "    sizes = [in_size] + hidden_sizes + [out_size]\n",
    "    net = []\n",
    "    for i in range(len(sizes) - 1):\n",
    "        net.append(torch.nn.Linear(\n",
    "            sizes[i], sizes[i+1]))\n",
    "        if i != len(sizes) - 2:\n",
    "            net.append(fabrics.make_activation(inner_act))\n",
    "            continue\n",
    "        else:\n",
    "            if fabrics.make_activation(final_act):\n",
    "                net.append(fabrics.make_activation(final_act))\n",
    "\n",
    "    return torch.nn.Sequential(*net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_loss_sample(\n",
    "    score: torch.nn.Module,\n",
    "    x: torch.tensor,\n",
    "    xi: torch.tensor,\n",
    "    sigma: float\n",
    ") -> float:\n",
    "    score_val = score(x + sigma*xi)\n",
    "    return torch.sum(score_val**2) + 2*torch.sum(score_val*xi) / sigma\n",
    "\n",
    "batch_score_loss = vmap(score_loss_sample, in_dims=(None, 0, 0, None))\n",
    "\n",
    "score_loss = lambda score, xs, xis, sigma: torch.mean(batch_score_loss(score, xs, xis, sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training loop\n",
    "def train_step(\n",
    "    score: torch.nn.Module,\n",
    "    bs: int,\n",
    "    sigma: float,\n",
    "    opt,\n",
    "    sched,\n",
    ") -> float:\n",
    "    \"\"\"Take a single optimization step on the denoising loss.\"\"\"\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # sample the target and gaussians\n",
    "    samples = rho.sample((bs,)).to(device) # [bs, d]\n",
    "    xis = torch.normal(0.0, 1.0, (bs, samples.shape[1])).to(device)\n",
    "\n",
    "    # compute the loss\n",
    "    loss_value = score_loss(score, samples, xis, sigma)\n",
    "\n",
    "    # compute the gradient\n",
    "    loss_value.backward()\n",
    "    grad_norm = torch.tensor([torch.nn.utils.clip_grad_norm_(score.parameters(), float('inf'))])\n",
    "\n",
    "    # perform the update.\n",
    "    opt.step()\n",
    "    sched.step()\n",
    "\n",
    "    return loss_value.detach(), grad_norm.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [128, 128, 128, 128]\n",
    "in_size = 2\n",
    "out_size = 2\n",
    "act = 'silu'\n",
    "base_lr = 1e-3\n",
    "sigma = 1e-1\n",
    "net = make_fc_net(hidden_sizes, in_size, out_size, inner_act=act, final_act=None).to(device)\n",
    "opt = torch.optim.Adam(net.parameters(), lr=base_lr)\n",
    "sched = torch.optim.lr_scheduler.StepLR(optimizer=opt, step_size=1000, gamma=0.8)\n",
    "nsteps = int(1e4)\n",
    "bs = int(1e5)\n",
    "plot_freq = 500   # how often to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'losses': torch.zeros(nsteps),\n",
    "    'grads': torch.zeros(nsteps)\n",
    "}\n",
    "\n",
    "for curr_step in range(nsteps):\n",
    "    s_loss, s_grad_norm = train_step(net, bs, sigma, opt, sched)\n",
    "    data_dict['losses'][curr_step] = s_loss\n",
    "    data_dict['grads'][curr_step] = s_grad_norm\n",
    "\n",
    "    if (curr_step % plot_freq) == 0:\n",
    "        print(s_loss, s_grad_norm)\n",
    "#         make_plots(s, interpolant, n_save, n_step, plot_bs, counter, metrics_freq, eps, data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a meshgrid\n",
    "x = torch.linspace(-10, 10, 25)\n",
    "y = torch.linspace(-10, 10, 25)\n",
    "xx, yy = torch.meshgrid(x, y)\n",
    "grid = torch.stack([xx.flatten(), yy.flatten()], dim=1).to(device)\n",
    "print(grid.shape)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Evaluate the function on the grid\n",
    "    results = vmap(net)(grid)\n",
    "\n",
    "    # Reshape for plotting\n",
    "    u = results[:, 0].reshape(xx.shape)\n",
    "    v = results[:, 1].reshape(yy.shape)\n",
    "\n",
    "    # Plotting\n",
    "    plt.scatter(rho0_samples[:,0], rho0_samples[:,1], s = 2, alpha = 0.2)\n",
    "    plt.scatter(rho1_samples[:,0], rho1_samples[:,1], s = 2, alpha = 0.2)\n",
    "    plt.quiver(grab(xx), grab(yy), grab(u), grab(v), zorder=1)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Vector Field of s(x)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String with learned score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.linspace(0,1,100).to(device)\n",
    "string = torch.stack([x0*(1-t) + t*(x1) for t in ts]).to(device)\n",
    "n_steps = 1000\n",
    "dt = .1\n",
    "\n",
    "strings_over_time = []\n",
    "new_string = string\n",
    "strings_over_time.append(new_string)\n",
    "for step in range(n_steps):\n",
    "    new_string = step_string(new_string, vmap(net), dt)\n",
    "    strings_over_time.append(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.contourf(X, Y, Z, levels=50, cmap='viridis')\n",
    "plt.colorbar(label='Log probability')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Contour Plot of Log Probability Function')\n",
    "\n",
    "\n",
    "color_sequence = create_color_sequence(start_color, end_color, len(strings_over_time))\n",
    "for i, string in enumerate(strings_over_time):\n",
    "    string = grab(string)\n",
    "    # print(string)\n",
    "    plt.scatter(string[:,0], string[:,1], color=color_sequence[i], alpha=0.5, s = 1)\n",
    "        \n",
    "        \n",
    "plt.scatter(grab(x0[0]), grab(x0[1]), label = 'x0', s = 10, color = 'blue')\n",
    "plt.scatter(grab(x1[0]), grab(x1[1]), label = 'x1', s = 10, color = 'blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load checkpoint for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "yangs_list = torch.load('yang_checks/mnist/checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "yangs_net, yangs_opt = yangs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = torch.normal(0.0, 1.0, (1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[205], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m yangs_net(test_image)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ]
    }
   ],
   "source": [
    "yangs_net(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_12_15_23",
   "language": "python",
   "name": "torch_12_15_23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
